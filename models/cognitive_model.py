# -*- coding: utf-8 -*-
"""Untitled48.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1T4tbhKQnQtjA4cE5rGxtNziShyJgHF8G
"""

import pandas as pd

diag = pd.read_csv("Diagnostic_Summary.csv")
mmse = pd.read_csv("MMSE.csv")
cdr = pd.read_csv("CDR.csv")
adas = pd.read_csv("ADAS.csv")
moca = pd.read_csv("MOCA.csv")

import pandas as pd

diag = pd.read_csv("Diagnostic_Summary.csv")
print(diag.columns)

print(mmse.columns)  # For MMSE.csv

print(cdr.columns)   # For CDR.csv

print(adas.columns)  # For ADAS.csv

print(moca.columns)  # For MOCA.csv

faq = pd.read_csv("FAQ.csv")
print(faq.columns)

neuro = pd.read_csv("Neuropsychological_Battery.csv")
print(neuro.columns)

uw = pd.read_csv("UW_Neuropsych_Summary_Scores.csv")
print(uw.columns)

ecog = pd.read_csv("Everyday_Cognition_Study_Partner_Report.csv")
print(ecog.columns)

import pandas as pd

# ==============================
# 1Ô∏è‚É£ LOAD ALL CSV FILES
# ==============================

mmse = pd.read_csv("MMSE.csv")
cdr = pd.read_csv("CDR.csv")
adas = pd.read_csv("ADAS.csv")
moca = pd.read_csv("MOCA.csv")
diag = pd.read_csv("Diagnostic_Summary.csv")


# ==============================
# 2Ô∏è‚É£ KEEP ONLY REQUIRED COLUMNS
# ==============================

mmse = mmse[['RID', 'VISCODE', 'MMSCORE']]
cdr = cdr[['RID', 'VISCODE', 'CDRSB']]
adas = adas[['RID', 'VISCODE', 'TOTAL13']]
moca = moca[['RID', 'VISCODE', 'MOCA']]
diag = diag[['RID', 'VISCODE', 'DIAGNOSIS']]


# ==============================
# 3Ô∏è‚É£ RENAME SCORE COLUMNS
# ==============================

mmse = mmse.rename(columns={'MMSCORE': 'MMSE'})
cdr = cdr.rename(columns={'CDRSB': 'CDR'})
adas = adas.rename(columns={'TOTAL13': 'ADAS13'})


# ==============================
# 4Ô∏è‚É£ MERGE ALL FILES
# ==============================

merged = mmse.merge(cdr, on=['RID', 'VISCODE'], how='outer') \
             .merge(adas, on=['RID', 'VISCODE'], how='outer') \
             .merge(moca, on=['RID', 'VISCODE'], how='outer')

final_data = merged.merge(diag, on=['RID', 'VISCODE'], how='left')


# ==============================
# 5Ô∏è‚É£ CHECK FINAL DATA
# ==============================

print(final_data.head())
print("Final shape:", final_data.shape)


# ==============================
# 6Ô∏è‚É£ (OPTIONAL) SAVE FINAL CSV
# ==============================

final_data.to_csv("Final_Merged_Data.csv", index=False)

# Load your final merged dataset
df = pd.read_csv("Final_Merged_Data.csv")

# 1Ô∏è‚É£ Remove MOCA column
df = df.drop(columns=["MOCA"])

# 2Ô∏è‚É£ Keep only rows where ALL remaining columns are filled
df_clean = df.dropna()

# 3Ô∏è‚É£ Reset index (optional but good practice)
df_clean = df_clean.reset_index(drop=True)

# 4Ô∏è‚É£ Check result
print("Shape before cleaning:", df.shape)
print("Shape after cleaning:", df_clean.shape)

print(df_clean.head())

# 5Ô∏è‚É£ Save clean dataset
df_clean.to_csv("Final_Cleaned_Data.csv", index=False)

"""**After Added Scores**"""

faq = faq[['RID', 'VISCODE', 'FAQTOTAL']]
neuro = neuro[['RID', 'VISCODE',
               'LDELTOTAL',
               'AVTOT1','AVTOT2','AVTOT3','AVTOT4','AVTOT5',
               'AVDELTOT']]
uw = uw[['RID', 'VISCODE', 'ADNI_MEM', 'ADNI_EF']]
ecog = ecog[['RID', 'VISCODE', 'EcogSPTotal']]

# Start from your cleaned dataset
df = pd.read_csv("Final_Cleaned_Data.csv")

# Merge FAQ
df = df.merge(faq, on=['RID','VISCODE'], how='left')

# Merge Neuropsych
df = df.merge(neuro, on=['RID','VISCODE'], how='left')

# Merge UW composite
df = df.merge(uw, on=['RID','VISCODE'], how='left')

# Merge Ecog
# df = df.merge(ecog, on=['RID','VISCODE'], how='left')
df = df.dropna()
# Remove rows with missing values
print("Shape after cleaning:", df.shape)

df.to_csv("Final_DL_Strong_Dataset.csv", index=False)

# ============================================
# 1Ô∏è‚É£ Import Libraries
# ============================================
import pandas as pd
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import classification_report, confusion_matrix


# ============================================
# 2Ô∏è‚É£ Load Dataset
# ============================================
df = pd.read_csv("Final_DL_Strong_Dataset.csv")

# Remove non-feature columns
df = df.drop(columns=["RID", "VISCODE"])


# ============================================
# 3Ô∏è‚É£ FIX ADNI DIAGNOSIS LABELS
# ADNI uses:
# 1 = CN
# 2 = MCI
# 3 = AD
# We must convert to:
# 0 = CN
# 1 = MCI
# 2 = AD
# ============================================

df['DIAGNOSIS'] = df['DIAGNOSIS'].map({
    1: 0,
    2: 1,
    3: 2
})

print("Class Distribution:")
print(df['DIAGNOSIS'].value_counts())


# ============================================
# 4Ô∏è‚É£ Separate Features and Target
# ============================================
X = df.drop(columns=["DIAGNOSIS"])
y = df["DIAGNOSIS"]


# ============================================
# 5Ô∏è‚É£ Train-Test Split (Stratified)
# ============================================
X_train, X_test, y_train, y_test = train_test_split(
    X, y,
    test_size=0.2,
    random_state=42,
    stratify=y
)


# ============================================
# 6Ô∏è‚É£ Normalize Features (Correct Way)
# Fit ONLY on training data
# ============================================
scaler = StandardScaler()

X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)


# ============================================
# 7Ô∏è‚É£ Convert to PyTorch Tensors
# ============================================
X_train_tensor = torch.tensor(X_train, dtype=torch.float32)
X_test_tensor = torch.tensor(X_test, dtype=torch.float32)

y_train_tensor = torch.tensor(y_train.values, dtype=torch.long)
y_test_tensor = torch.tensor(y_test.values, dtype=torch.long)


# ============================================
# 8Ô∏è‚É£ Define Neural Network
# ============================================
class AlzheimerNN(nn.Module):
    def __init__(self, input_size):
        super(AlzheimerNN, self).__init__()

        self.model = nn.Sequential(
            nn.Linear(input_size, 64),
            nn.ReLU(),
            nn.BatchNorm1d(64),
            nn.Dropout(0.3),

            nn.Linear(64, 32),
            nn.ReLU(),
            nn.BatchNorm1d(32),
            nn.Dropout(0.3),

            nn.Linear(32, 16),
            nn.ReLU(),

            nn.Linear(16, 3)  # 3 output classes (CN, MCI, AD)
        )

    def forward(self, x):
        return self.model(x)


# Initialize model
input_size = X_train.shape[1]
model = AlzheimerNN(input_size)



import torch

# Manually calculated weights
class_weights = torch.tensor([0.89, 0.81, 1.58], dtype=torch.float32)

# Move to device if using GPU
# class_weights = class_weights.to(device)

criterion = nn.CrossEntropyLoss(weight=class_weights)

# ============================================
# 9Ô∏è‚É£ Loss Function & Optimizer
# ============================================
# criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=0.001)


# ============================================
# üîü Training Loop
# ============================================
num_epochs = 500

for epoch in range(num_epochs):
    model.train()

    optimizer.zero_grad()
    outputs = model(X_train_tensor)
    loss = criterion(outputs, y_train_tensor)
    loss.backward()
    optimizer.step()

    if (epoch+1) % 10 == 0:
        print(f"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}")


# ============================================
# 1Ô∏è‚É£1Ô∏è‚É£ Evaluation
# ============================================
model.eval()

with torch.no_grad():
    outputs = model(X_test_tensor)
    _, predicted = torch.max(outputs, 1)

    accuracy = (predicted == y_test_tensor).sum().item() / y_test_tensor.size(0)

print("\nTest Accuracy:", round(accuracy * 100, 2), "%")

print("\nClassification Report:")
print(classification_report(y_test_tensor, predicted))

print("\nConfusion Matrix:")
print(confusion_matrix(y_test_tensor, predicted))